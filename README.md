-Features-

-FastAPI backend serving LLM responses locally
-API key authentication and credit-based access control
-Environment variable management using .env and python-dotenv
-Seamless model interaction with Ollama
-JSON-formatted response output for easy integration into other tools or front-ends



-Tech Stack-

-Language: Python 3.10+
-Framework: FastAPI
-LLM Backend: Ollama (Gemma 2B)
-Environment Management: dotenv
-API Security: Header-based API keys



-What I Learned-

-How to build and deploy REST APIs using FastAPI
-Managing secure API keys and request authorization
-Working with Ollamaâ€™s local LLM inference
-Structuring clean, readable, and maintainable backend code
-Designing credit-based API usage logic



-Future Improvements-

-Add rate limiting and usage tracking per user
-Implement frontend integration for web-based access
-Extend support for multiple Ollama models
